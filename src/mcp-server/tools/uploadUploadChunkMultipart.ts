/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod";
import { uploadUploadChunkMultipart } from "../../funcs/uploadUploadChunkMultipart.js";
import * as components from "../../models/components/index.js";
import { formatResult, ToolDefinition } from "../tools.js";

const args = {
  resourceType: components.UploadResourceType$inboundSchema.default("auto"),
  contentRange: z.string(),
  xUniqueUploadId: z.string(),
  binaryUploadRequest: components.BinaryUploadRequest$inboundSchema,
};

export const tool$uploadUploadChunkMultipart: ToolDefinition<typeof args> = {
  name: "upload-upload-chunk-multipart",
  description: `Upload a single chunk of a large file

Uploads a single chunk of a large file as part of a chunked upload process. This enables efficient upload of 
large files with the ability to resume interrupted uploads. Each request uploads one chunk of the file.
It is required for any files that are larger than 100 MB. This is often relevant for video files, as they 
tend to have larger file sizes. Minimum chunk size is 5 MB.
`,
  scopes: ["librarian"],
  args,
  tool: async (client, args, ctx) => {
    const [result, apiCall] = await uploadUploadChunkMultipart(
      client,
      args.resourceType,
      args.contentRange,
      args.xUniqueUploadId,
      args.binaryUploadRequest,
      { fetchOptions: { signal: ctx.signal } },
    ).$inspect();

    if (!result.ok) {
      return {
        content: [{ type: "text", text: result.error.message }],
        isError: true,
      };
    }

    const value = result.value;

    return formatResult(value, apiCall);
  },
};
